{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Equilibrium Nets for Guvenen (2009)  TF1\n",
    "\n",
    "# Log Utility:  $\\frac{\\Lambda_{i,t+1}}{\\Lambda_{i,t}}=\\frac{c_{i,t}}{c_{i,t+1}}$\n",
    "\n",
    "#### By  Matias Covarrubias and Min Fang\n",
    "\n",
    "In this notebook, we use `TensorFlow` to solve [Guvenen (2009)](http://users.econ.umn.edu/~guvenen/HABHET2008.pdf) with _deep equilibrium nets_ method by [Azinovic, Gaegauf, & Scheidegger (2020)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3393482).\n",
    "\n",
    "\n",
    "## The model <a id='model'></a>\n",
    "\n",
    "GDSGE offers a good [summary](http://www.gdsge.com/example/Guvenen2009/Guvenen2009.html).\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "0. [Set up workspace](#workspace)\n",
    "1. [Model calibration](#modelcal)\n",
    "2. [_Deep equilibrium net_ hyper-parameters](#deqnparam)\n",
    "    1. [Neural network](#nn)\n",
    "3. [Economic model](#econmodel)\n",
    "    1. [Current period (t)](#currentperiod)\n",
    "    2. [Next period (t+1)](#nextperiod)\n",
    "    3. [Cost/Euler function](#cost)\n",
    "4. [Training](#training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Set up workspace <a id='workspace'></a>\n",
    "\n",
    "First, we need to set up the workspace. All of the packages are standard python packages. This version of the _deep equilibrium net_ notebook will be computed with `TensorFlow 1.13.1`. Make sure you are working in an environment with TF1 installed. Use `pip install tensorflow==1.13.1` to install the correct version.\n",
    "\n",
    "The only special module is `utils` from which we import a mini-batch function `random_mini_batches` and a function that initializes the neural network weights `initialize_nn_weight`.\n",
    "\n",
    "### Saving and continuing training\n",
    "\n",
    "You can save and resume training by saving and loading the tensorflow session and data starting point.\n",
    "* The saved session stores the neural network weights and the optimizer's state. If you have saved a session that you would like to reload, set `sess_path` to the session checkpoint path. For example, to load the 100th episode's session set `sess_path = './output/sess_100.ckpt'`. Otherwise, set `sess_path` to `None` to train from scratch. Currently, this script saves the session at the end of each [episode](#deqnparam).\n",
    "* The saved data starting point stores the an exogeneous shock and a capital distribution, which can be used to simulate states into the future from. If you have saved a starting point that you would like to reload, set `data_path` to the numpy data path. For example, to load the 100th episode's starting point set `data_path = './output/data_100.npy'`. Otherwise, set `data_path` to `None` to train from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# Import modules\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from utils import initialize_nn_weight, random_mini_batches\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rc('xtick', labelsize='small')\n",
    "plt.rc('ytick', labelsize='small')\n",
    "std_figsize = (4, 4)\n",
    "\n",
    "# Make sure that we are working with tensorflow 1\n",
    "print('tf version:', tf.__version__)\n",
    "assert tf.__version__[0] == '1'\n",
    "\n",
    "# Set the seed for replicable results\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Helper variables\n",
    "eps = 0.00001  # Small epsilon value\n",
    "\n",
    "# Make output directory to save network weights and starting point\n",
    "if not os.path.exists('./output'):\n",
    "    os.mkdir('./output')\n",
    "\n",
    "# Path to saved tensorflow session\n",
    "sess_path = None\n",
    "# Path to saved data starting point\n",
    "data_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model calibration <a id='modelcal'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parameters\n",
    "alpha = 6\n",
    "alpha_tf = tf.constant(alpha)\n",
    "beta = 0.9966\n",
    "beta_tf = tf.constant(beta)\n",
    "theta = 0.3\n",
    "theta_tf = tf.constant(theta)\n",
    "rho_h = 1/0.3\n",
    "tho_h_tf = tf.constant(rho_h)\n",
    "rho_n = 1/0.1\n",
    "rho_n_tf = tf.constant(rho_n)\n",
    "delta = 0.0066\n",
    "delta_tf = tf.constant(delta)\n",
    "mu = 0.2\n",
    "mu_tf = tf.constant(mu)\n",
    "phi_k = 0.4\n",
    "phi_k_tf = tf.constant(phi_k)\n",
    "chi = 0.005\n",
    "chi_tf = tf.constant(phi_k)\n",
    "Kbar = ((1/beta - 1 + delta)/theta)**(1/(theta-1))\n",
    "Bbar = -0.1*(1-theta)*Kbar**theta\n",
    "Kbar_tf = tf.constant(Kbar)\n",
    "Bbar_tf = tf.constant(Bbar)\n",
    "\n",
    "a1 = (((delta**(1/phi_k))*phi_k)/(phi_k-1));\n",
    "a2 = (delta/(1-phi_k));\n",
    "a1_tf = tf.constant(a1)\n",
    "a2_tf = tf.constant(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exodogeneous TFP shock\n",
    "# import quantecon\n",
    "# phi_z = 0.9\n",
    "# sigma_z = 0.1\n",
    "# x = quantecon.tauchen(phi_z,sigma_z,n=2)\n",
    "# Pi = x.P\n",
    "# Zgrid = np.exp(x.state_values)\n",
    "Pi_np = 0.5 * np.ones((2, 2))\n",
    "Pi_tf = tf.constant(0.5 * np.ones((2, 2)), dtype=tf.float32)\n",
    "Zgrid_tf = tf.constant([[0.95], [1.05]], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. _Deep equilibrium net_ hyper-parameters <a id='deqnparam'></a>\n",
    "\n",
    "keep as less as control variables: \n",
    "\n",
    "2 endogeneous states: Kx, Bnx;\n",
    "\n",
    "1 exogeneous state: Zx;\n",
    "\n",
    "8 controls: \n",
    "\n",
    "Core: chy, cny, Iy, Bny, lambdahy, lambdany, Psy, Pfy;\n",
    "\n",
    "equations: (1) ---> (7) + LOM of K + LOM of Z (18)\n",
    "\n",
    "LOM of K: Knext = (1-delta)*K + (a1*((Inv/K)^((xsi-1)/xsi))+a2)*K;\n",
    "\n",
    "a1 = (((delta^(1/xsi))*xsi)/(xsi-1));\n",
    "a2 = (delta/(1-xsi));\n",
    "\n",
    "rewrite: \n",
    "\n",
    "$b_{n,t+1} = B^n_{t+1}/(1-\\mu)$\n",
    "\n",
    "$b_{h,t+1} = (\\chi\\bar{K} - B^n_{t+1})/\\mu$\n",
    "\n",
    "$W_t = (1-\\theta) Z_t K_t^\\theta$\n",
    "\n",
    "$D_t = Y_t - W_t - I_t - (1-P^f_t)\\chi\\bar{K}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000 \n",
    "len_episodes = 10240\n",
    "epochs_per_episode = 20\n",
    "minibatch_size = 512\n",
    "num_minibatches = int(len_episodes/minibatch_size)\n",
    "lr = 0.00001\n",
    "\n",
    "#Â Neural network architecture parameters\n",
    "num_input_nodes = 3           # Dimension of extended state space \n",
    "num_hidden_nodes = [100, 50]  # Dimension of hidden layers\n",
    "num_output_nodes = 8          # Output dimension "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.A. Neural network <a id='nn'></a>\n",
    "\n",
    "Since we are using a neural network with 2 hidden layers, the network maps:\n",
    "$$X \\rightarrow \\mathcal{N}(X) = \\sigma(\\sigma(XW_1 + b_1)W_2 + b_2)W_3 + b_3$$\n",
    "where $\\sigma$ is the rectified linear unit (ReLU) activation function and the output layer is activated with the linear function (which is the identity function and hence omitted from the equation). Therefore, we need 3 weight matrices $\\{W_1, W_2, W_3\\}$ and 3 bias vectors $\\{b_1, b_2, b_3\\}$ (compare with the neural network diagram above). In total, we train 5'954 parameters:\n",
    "\n",
    "| $W_1$ | $3 \\times 100$  | $= 300$ |\n",
    "| --- | --- | --- |\n",
    "| $W_2$ | $100 \\times 50$ | $= 5000$ |\n",
    "| $W_3$ | $50 \\times 8$ | $= 400$ |\n",
    "| $b_1 + b_2 + b_3$ | $100 + 50 + 8$ | $= 158$|\n",
    "\n",
    "\n",
    "We initialize the neural network parameters with the  `initialize_nn_weight` helper function from `utils`.\n",
    "\n",
    "Then, we compute the neural network prediction using the parameters in `nn_predict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# We create a placeholder for X, the input data for the neural network, which corresponds\n",
    "# to the state.\n",
    "X = tf.placeholder(tf.float32, shape=(None, num_input_nodes))\n",
    "# Get number samples\n",
    "m = tf.shape(X)[0]\n",
    "\n",
    "# We create all of the neural network weights and biases. The weights are matrices that\n",
    "# connect the layers of the neural network. For example, W1 connects the input layer to\n",
    "# the first hidden layer\n",
    "W1 = initialize_nn_weight([num_input_nodes, num_hidden_nodes[0]])\n",
    "W2 = initialize_nn_weight([num_hidden_nodes[0], num_hidden_nodes[1]])\n",
    "W3 = initialize_nn_weight([num_hidden_nodes[1], num_output_nodes])\n",
    "\n",
    "# The biases are extra (shift) terms that are added to each node in the neural network.\n",
    "b1 = initialize_nn_weight([num_hidden_nodes[0]])\n",
    "b2 = initialize_nn_weight([num_hidden_nodes[1]])\n",
    "b3 = initialize_nn_weight([num_output_nodes])\n",
    "\n",
    "# Then, we create a function, to which we pass X, that generates a prediction based on\n",
    "# the current neural network weights. Note that the hidden layers are ReLU activated.\n",
    "# The output layer is not activated (i.e., it is activated with the linear function).\n",
    "def nn_predict(X):\n",
    "    hidden_layer1 = tf.nn.relu(tf.add(tf.matmul(X, W1), b1))\n",
    "    hidden_layer2 = tf.nn.relu(tf.add(tf.matmul(hidden_layer1, W2), b2))\n",
    "    output_layer = tf.add(tf.matmul(hidden_layer2, W3), b3)\n",
    "    return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Economic model <a id='econmodel'></a>\n",
    "\n",
    "In this section, we implement the economics outlined in [the model](#model).\n",
    "\n",
    "Each period, based on the current distribution of capital and the exogenous state, agents decide whether and how much to save in risky capital and to consume. Their savings together with the labor supplied implies the rest of the economic state (e.g., capital return, wages, incomes, ...). We use the neural network to generate the savings based on the agents' capital holdings at the beginning of the period. The remaining economic state is computed using the equations outlined [above](#model). The economic mechanisms are encoded in helper functions. We create one for the `firm`, the `shocks`, and `wealth` (see cell below).\n",
    "\n",
    "Then, in the face of future uncertainty, the agents again decide whether and how much to save in risky capital for the next period. We use the neural network to generate new savings for each of the future states (one for each shock). The input state for these network predictions is the next period's capital holding which are current periods savings.\n",
    "\n",
    "First, we define the helper functions for the `firm`, the `shocks`, and `wealth`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper(Zx, Kx, Bnx, Iy, Pfy):\n",
    "    if Zx == 1:\n",
    "      Z = np.exp(0.95)\n",
    "    else:\n",
    "      Z = np.exp(1.05)\n",
    "    Output = Z * (Kx**theta)\n",
    "    W = (1-theta) * Z * (Kx**theta)\n",
    "    D = Output - W - Iy - (1-Pfy)*chi_tf*Kbar_tf\n",
    "    return Output, W, D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.A. Current period (t) <a id='currentperiod'></a>\n",
    "Using the current state `X` we can calculate the economy. The state is composed of today's shock ($Z_t$), today's capital ($K_t$), and today's bond position ($B_t^n$). \n",
    "\n",
    "Using the current state `X`, we predict the controls. We do this by passing the state `X` to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Today's extended state: \n",
    "Zx = X[:, 0]  # exogenous shock\n",
    "Kx = X[:, 1]  # aggregate capital\n",
    "Bnx = X[:, 2] # bond holding of nonstockholders\n",
    "\n",
    "# Today's controls\n",
    "Y = nn_predict(X)\n",
    "# dimensions of Y: chy, cny, Iy, Bny, lambdahy, lambdany, Psy, Pfy\n",
    "chy = Y[:,0]\n",
    "cny = Y[:,1]\n",
    "Iy = Y[:,2]\n",
    "Bny = Y[:,3]\n",
    "lambdahy = Y[:,4]\n",
    "lambdany = Y[:,5]\n",
    "Psy = Y[:,6]\n",
    "Pfy = Y[:,7]\n",
    "\n",
    "Output, W, D = helper(Zx, Kx, Bnx, Iy, Pfy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.B. Next period (t+1) <a id='nextperiod'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Today's control Bny becomes tomorrow's state Bnx\n",
    "Bnx_prime = Bny\n",
    "\n",
    "# LOM of Capital\n",
    "Kx_prime = (1-delta_tf)*Kx + (a1_tf*(Iy/Kx)**((phi_k_tf-1)/phi_k_tf)+a2_tf)*Kx\n",
    "\n",
    "# Shock 1 [0.95] ---------------------------------------------------------------------\n",
    "# 1) Get remaining parts of tomorrow's extended state\n",
    "# Exogenous shock\n",
    "Z_prime_1 = 0.95 * tf.ones_like(Zx)\n",
    "\n",
    "# Tomorrow's state: Concatenate the parts together\n",
    "X_prime_1 = tf.concat([tf.expand_dims(Z_prime_1, 1), \n",
    "                       tf.expand_dims(Kx_prime, 1),\n",
    "                       tf.expand_dims(Bnx_prime, 1)], axis=1)\n",
    "\n",
    "# 2) Get tomorrow's policy\n",
    "# Tomorrow's capital: capital holding at beginning of period and how much they save\n",
    "Y_prime_1 = nn_predict(X_prime_1)\n",
    "\n",
    "Output_prime1, W_prime1, D_prime1 = helper(Z_prime_1, Kx_prime, Bnx_prime, Y_prime_1[:,2], Y_prime_1[:,7])\n",
    "\n",
    "\n",
    "# Shock 2 [1.05] ---------------------------------------------------------------------\n",
    "# 1) Get remaining parts of tomorrow's extended state\n",
    "# Exogenous shock\n",
    "Z_prime_2 = 1.05 * tf.ones_like(Zx)\n",
    "\n",
    "# Tomorrow's state: Concatenate the parts together\n",
    "X_prime_2 = tf.concat([tf.expand_dims(Z_prime_2, 1), \n",
    "                       tf.expand_dims(Kx_prime, 1),\n",
    "                       tf.expand_dims(Bnx_prime, 1)], axis=1)\n",
    "\n",
    "# 2) Get tomorrow's policy\n",
    "# Tomorrow's capital: capital holding at beginning of period and how much they save\n",
    "Y_prime_2 = nn_predict(X_prime_2)\n",
    "\n",
    "Output_prime2, W_prime2, D_prime2 = helper(Z_prime_2, Kx_prime, Bnx_prime, Y_prime_2[:,2], Y_prime_2[:,7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.C. Cost / Euler function <a id='cost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Put our equilibrium conditions\n",
    "  err_bdgt_h = 1 - (W + (Div/mu) + b_h - Pf*(chi*Kss*(1-bn_shr_next)/mu))/c_h; % these are individual consumptions\n",
    "  err_bdgt_n = 1 - (W + b_n - Pf*(bn_shr_next*chi*Kss/(1-mu)))/c_n;\n",
    "  foc_stock = 1 - (beta*EEulerstock_future*(Evh_future^((alpha-rhoh)/(1-alpha))))/((c_h^(-rhoh))*Ps);\n",
    "  foc_bondh = 1 - (beta*EEulerbondh_future*(Evh_future^((alpha-rhoh)/(1-alpha))) + lambdah)/((c_h^(-rhoh))*Pf);\n",
    "  foc_bondn = 1 - (beta*EEulerbondn_future*(Evn_future^((alpha-rhon)/(1-alpha))) + lambdan)/((c_n^-rhon)*Pf);\n",
    "  foc_f = 1 - (beta*EEulerf_future*(Evh_future^((alpha-rhoh)/(1-alpha))))/((c_h^(-rhoh))*dIdKp);\n",
    "  \n",
    "  slack_bn = lambdan*(bn_shr_next - bn_shr_lb);    %mun_lw*bn_shr_next;\n",
    "  slack_bh = lambdah*(bn_shr_ub - bn_shr_next);    %mun_up*(1-bn_shr_next);\n",
    "  \n",
    "  ALSO: try weights for the functions\"\"\"\n",
    "\n",
    "# Prepare transitions to the next periods states. In this setting, there is a 25% chance\n",
    "# of ending up in any of the 4 states in Z. This has been hardcoded and need to be changed\n",
    "# to accomodate a different transition matrix.\n",
    "pi_trans_to1 = 0.5 * tf.ones((m, 1))\n",
    "pi_trans_to2 = 0.5 * tf.ones((m, 1))\n",
    "\n",
    "# EQM equation (1)\n",
    "opt_eq1 = -Pfy + beta_tf * (1+lambdahy) * chy * ( pi_trans_to1/Y_prime_1[:,1] + pi_trans_to2/Y_prime_2[:,1])\n",
    "\n",
    "# EQM equation (2)\n",
    "opt_eq2 = -Pfy + beta_tf * (1+lambdany) * cny * ( pi_trans_to1/Y_prime_1[:,2] + pi_trans_to2/Y_prime_2[:,2])\n",
    "\n",
    "# EQM equation (3)\n",
    "opt_eq3 = -Psy + beta_tf * cny * ( pi_trans_to1 * (Y_prime_1[:,6] + D_prime1) / Y_prime_1[:,2]                                \n",
    "                              + pi_trans_to2 * (Y_prime_2[:,6] + D_prime2) / Y_prime_2[:,2] )\n",
    "\n",
    "# EQM equation (4)\n",
    "opt_eq4 = lambdahy * (Bbar_tf + (chi_tf*Kbar_tf - Bnx_prime)/mu_tf)\n",
    "\n",
    "# EQM equation (5)\n",
    "opt_eq5 = lambdahy * (Bbar_tf + Bnx_prime/(1-mu_tf))\n",
    "\n",
    "# EQM equation (6)\n",
    "opt_eq6 = ( chy + Pfy*((chi_tf*Kbar_tf - Bnx_prime)/mu_tf) + Psy/mu_tf\n",
    "            - ( Psy + D + (chi_tf*Kbar_tf - Bnx)/mu_tf + W)\n",
    "           )\n",
    "\n",
    "# EQM equation (7)\n",
    "opt_eq7 = ( cny + Pfy*Bnx_prime/(1-mu_tf)\n",
    "            - ( Psy + Bnx/(1-mu_tf) + W)\n",
    "           )\n",
    "\n",
    "# Punishment for negative consumption (c)\n",
    "orig_cons = tf.concat([tf.expand_dims(chy,1), tf.expand_dims(chy,1)], axis=1)\n",
    "opt_punish_cons = (1./eps) * tf.maximum(-1 * orig_cons, tf.zeros_like(orig_cons))\n",
    "\n",
    "# Punishment for negative aggregate capital (K)\n",
    "opt_punish_ktot_prime = (1./eps) * tf.maximum(-Kx_prime, tf.zeros_like(Kx_prime))\n",
    "\n",
    "# Concatenate the 3 equilibrium functions\n",
    "combined_opt = [opt_eq1,\n",
    "                opt_eq2,\n",
    "                opt_eq3, \n",
    "                tf.expand_dims(opt_eq4,1), \n",
    "                tf.expand_dims(opt_eq5,1), \n",
    "                tf.expand_dims(opt_eq6,1), \n",
    "                tf.expand_dims(opt_eq7,1), \n",
    "                opt_punish_cons, \n",
    "                tf.expand_dims(opt_punish_ktot_prime,1)]\n",
    "\n",
    "opt_predict = tf.concat(combined_opt, axis=1)\n",
    "\n",
    "# Define the \"correct\" outputs. For all equilibrium functions, the correct outputs is zero.\n",
    "opt_correct = tf.zeros_like(opt_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizer\n",
    "\n",
    "Next, we chose an optimizer; i.e., the algorithm we use to perform gradient descent. We use [Adam](https://arxiv.org/abs/1412.6980), a favorite in deep learning research. Adam uses a parameter specific learning rate and momentum, which encourages gradient descent steps that occur in a consistent direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/min/opt/anaconda3/envs/TF1/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Define the cost function\n",
    "cost = tf.losses.mean_squared_error(opt_correct, opt_predict)\n",
    "\n",
    "# Adam optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# Clip the gradients to limit the extent of exploding gradients\n",
    "gvs = optimizer.compute_gradients(cost)\n",
    "\n",
    "# Define a training step\n",
    "train_step = optimizer.apply_gradients(gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training <a id='training'></a>\n",
    "\n",
    "### tldr\n",
    "\n",
    "We iterate between simulating a dataset using the current neural network and training on this dataset.\n",
    "\n",
    "***\n",
    "\n",
    "In the final stage, we put everything together and train the neural network. \n",
    "\n",
    "In this section, we iterate between a simulation phase and a training phase. That is, we first simulate a dataset. We simulate a sequence of states ($[z, k]$) based on a random sequence of shocks. The states are computed using the current state of the neural network. We created the `simulate_episodes` helper function to do this. Then, in the training phase, we use the dataset to update our network parameters through multiple epochs. After completion of the training phase, we resimulate a dataset using the new network parameters and repeat.\n",
    "\n",
    "By computing the error directly after simulating a new dataset, we are able to evaluate our algorithms out-of-sample performance.\n",
    "\n",
    "First, we define the helper function `simulate_episodes` that simulates the training data used in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episodes(sess, x_start, episode_length, print_flag=True):\n",
    "    \"\"\"Simulate an episode for a given starting point using the current\n",
    "       neural network state.\n",
    "\n",
    "    Args:\n",
    "        sess: Current tensorflow session,\n",
    "        x_start: Starting state to simulate forward from,\n",
    "        episode_length: Number of steps to simulate forward,\n",
    "        print_flag: Boolean that determines whether to print simulation stats.\n",
    "\n",
    "    Returns:\n",
    "        X_episodes: Tensor of states [z, k] to train on (training set).\n",
    "    \"\"\"\n",
    "    time_start = datetime.now()\n",
    "    if print_flag:\n",
    "        print('Start simulating {} periods.'.format(episode_length))\n",
    "    dim_state = np.shape(x_start)[1]\n",
    "\n",
    "    X_episodes = np.zeros([episode_length, dim_state])\n",
    "    X_episodes[0, :] = x_start\n",
    "    X_old = x_start\n",
    "\n",
    "    # Generate a sequence of random shocks\n",
    "    rand_num = np.random.rand(episode_length, 1)\n",
    "\n",
    "    for t in range(1, episode_length):\n",
    "        z = int(X_old[0, 0])  # Current period's shock\n",
    "\n",
    "        # Determine which state we will be in in the next period based on\n",
    "        # the shock and generate the corresponding state (x_prime)\n",
    "        if rand_num[t - 1] <= Pi_np[z, 0]:\n",
    "            X_new = sess.run(X_prime_1, feed_dict={X: X_old})\n",
    "        else: \n",
    "            X_new = sess.run(X_prime_2, feed_dict={X: X_old})\n",
    "        \n",
    "        # Append it to the dataset\n",
    "        X_episodes[t, :] = X_new\n",
    "        X_old = X_new\n",
    "\n",
    "    time_end = datetime.now()\n",
    "    time_diff = time_end - time_start\n",
    "    if print_flag:\n",
    "        print('Finished simulation. Time for simulation: {}.'.format(time_diff))\n",
    "\n",
    "    return X_episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the _deep equilibrium net_\n",
    "\n",
    "Now we can begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated a valid starting point\n",
      "start time: 2021-02-18 21:49:39.012110\n",
      "Start simulating 10240 periods.\n",
      "Finished simulation. Time for simulation: 0:00:01.996678.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'opt_euler_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9ac82445fcde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# This way, the errors are calculated out-of-sample.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mopt_eq1_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_eq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminibatch_X\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mee_error\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_euler_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mmb_max_ee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_euler_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mmax_ee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_ee\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_max_ee\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opt_euler_' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize tensorflow session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Generate a random starting point\n",
    "if data_path:\n",
    "    X_data_train = np.load(data_path)\n",
    "    print('Loaded initial data from ' + data_path)\n",
    "    start_episode = int(re.search('_(.*).npy', data_path).group(1))\n",
    "else:\n",
    "    X_data_train = np.random.rand(1, num_input_nodes)\n",
    "    X_data_train[:, 0] = (X_data_train[:, 0] > 0.5)\n",
    "    X_data_train[:, 1:] = X_data_train[:, 1:] + 0.1\n",
    "    assert np.min(np.sum(X_data_train[:, 1:], axis=1, keepdims=True) > 0) == True, 'Starting point has negative aggregate capital (K)!'\n",
    "    print('Calculated a valid starting point')\n",
    "    start_episode = 0\n",
    "\n",
    "train_seed = 0\n",
    "\n",
    "cost_store, mov_ave_cost_store = [], []\n",
    "\n",
    "time_start = datetime.now()\n",
    "print('start time: {}'.format(time_start))\n",
    "\n",
    "#Â Initialize the random variables (neural network weights)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Initialize saver to save and load previous sessions\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "\n",
    "if sess_path is not None:\n",
    "    saver.restore(sess, sess_path)\n",
    "            \n",
    "for episode in range(start_episode, num_episodes):\n",
    "    # Simulate data: every episode uses a new training dataset generated on the current\n",
    "    # iteration's neural network parameters.\n",
    "    X_episodes = simulate_episodes(sess, X_data_train, len_episodes, print_flag=(episode==0))\n",
    "    X_data_train = X_episodes[-1, :].reshape([1, -1])\n",
    "    ee_error = np.zeros((1, num_output_nodes))\n",
    "\n",
    "    for epoch in range(epochs_per_episode):\n",
    "        # Every epoch is one full pass through the dataset. We train multiple passes on \n",
    "        # one training set before we resimulate a new dataset.\n",
    "        train_seed += 1\n",
    "        minibatch_cost = 0\n",
    "\n",
    "        # Mini-batch the simulated data\n",
    "        minibatches = random_mini_batches(X_episodes, minibatch_size, train_seed)\n",
    "\n",
    "        for minibatch_X in minibatches:\n",
    "            # Run optimization; i.e., determine the cost of each mini-batch.\n",
    "            minibatch_cost += sess.run(cost, feed_dict={X: minibatch_X}) / num_minibatches\n",
    "            if epoch == 0:\n",
    "                # For the first epoch, save the mean and max euler errors for plotting\n",
    "                # This way, the errors are calculated out-of-sample.\n",
    "                opt_eq1_ = np.abs(sess.run(opt_eq1, feed_dict={X: minibatch_X}))\n",
    "                ee_error += np.mean(opt_eq1_, axis=0) / num_minibatches\n",
    "                mb_max_ee = np.max(opt_eq1_, axis=0, keepdims=True)\n",
    "                max_ee = np.maximum(max_ee, mb_max_ee)\n",
    "\n",
    "        if epoch == 0:\n",
    "            # Record the cost and moving average of the cost at the beginning of each\n",
    "            # episode to track learning progress.\n",
    "            cost_store.append(minibatch_cost)\n",
    "            mov_ave_cost_store.append(np.mean(cost_store[-100:]))\n",
    "\n",
    "        for minibatch_X in minibatches:\n",
    "            # Take a mini-batch gradient descent training step. That is, update the\n",
    "            # weights for one mini-batch.\n",
    "            sess.run(train_step, feed_dict={X: minibatch_X})\n",
    "            \n",
    "  \n",
    "    #========================================================================================\n",
    "    # Print cost and time log\n",
    "    print('Episode {}: log10(Cost): {:.4f}; time: {}; time since start: {}'.format(episode, \n",
    "                                                                                   np.log10(cost_store[-1]), \n",
    "                                                                                   datetime.now().time(), \n",
    "                                                                                   datetime.now() - time_start))\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        # Save the tensorflow session\n",
    "        saver.save(sess, './output/sess_{}.ckpt'.format(episode))\n",
    "        # Save the starting point\n",
    "        np.save('./output/data_{}.npy'.format(episode), X_data_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
